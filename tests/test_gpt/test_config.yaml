defaults:
  - _self_

model:
  _target_: Cfg
  n_head: 4
  d_model: 512
  n_layer: 4
  dropout_p: 0.0
  context_length: 512
  weight_precision: bfloat16
  mlp_scale_factor: 4
  mlp_bias: True
  attn_bias: False
  proj_bias: True
  ln_bias: True
  cls_head_bias: True
  activation: relu

tokenizer:
  name: t5-base
  vocab_size: 32128