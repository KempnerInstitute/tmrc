{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "libcufile.so.*[0-9] not found in the system path ['/n/holylabs/LABS/sham_lab/Users/mkwun/tmrc/src/tmrc/tmrc_core/training', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python310.zip', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/lib-dynload', '', '/n/home05/mkwun/.local/lib/python3.10/site-packages', '__editable__.fschat-0.2.34.finder.__path_hook__', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/site-packages', '__editable__.ai2_olmo-0.4.0.finder.__path_hook__', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/site-packages/setuptools/_vendor']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/site-packages/torch/__init__.py:300\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 300\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_deps_lib_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRTLD_GLOBAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# Can only happen for wheel with cuda libs as PYPI deps\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# As PyTorch is not purelib, but nvidia-*-cu12 is\u001b[39;00m\n",
      "File \u001b[0;32m/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: libnvToolsExt.so.1: cannot open shared object file: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01molmo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m build_train_dataloader, MemMapDataset, DataCollator, IterableDataset\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01molmo\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m barrier, get_global_rank, get_world_size\n",
      "File \u001b[0;32m/n/holylabs/LABS/sham_lab/Users/mkwun/olmo_test/olmo/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/n/holylabs/LABS/sham_lab/Users/mkwun/olmo_test/olmo/config.py:21\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     Any,\n\u001b[1;32m      9\u001b[0m     Dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     cast,\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DictConfig, ListConfig\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OmegaConf \u001b[38;5;28;01mas\u001b[39;00m om\n",
      "File \u001b[0;32m/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/site-packages/torch/__init__.py:367\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# Easy way.  You want this most of the time, because it will prevent\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# C++ symbols from libtorch clobbering C++ symbols from other\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;66;03m# See Note [Global dependencies]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[0;32m--> 367\u001b[0m         \u001b[43m_load_global_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSymInt\u001b[39;00m:\n",
      "File \u001b[0;32m/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/site-packages/torch/__init__.py:325\u001b[0m, in \u001b[0;36m_load_global_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m lib_folder, lib_name \u001b[38;5;129;01min\u001b[39;00m cuda_libs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 325\u001b[0m     \u001b[43m_preload_cuda_deps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlib_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m ctypes\u001b[38;5;241m.\u001b[39mCDLL(global_deps_lib_path, mode\u001b[38;5;241m=\u001b[39mctypes\u001b[38;5;241m.\u001b[39mRTLD_GLOBAL)\n",
      "File \u001b[0;32m/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/site-packages/torch/__init__.py:284\u001b[0m, in \u001b[0;36m_preload_cuda_deps\u001b[0;34m(lib_folder, lib_name)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib_path:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlib_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in the system path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    285\u001b[0m ctypes\u001b[38;5;241m.\u001b[39mCDLL(lib_path)\n",
      "\u001b[0;31mValueError\u001b[0m: libcufile.so.*[0-9] not found in the system path ['/n/holylabs/LABS/sham_lab/Users/mkwun/tmrc/src/tmrc/tmrc_core/training', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python310.zip', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/lib-dynload', '', '/n/home05/mkwun/.local/lib/python3.10/site-packages', '__editable__.fschat-0.2.34.finder.__path_hook__', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/site-packages', '__editable__.ai2_olmo-0.4.0.finder.__path_hook__', '/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/site-packages/setuptools/_vendor']"
     ]
    }
   ],
   "source": [
    "from olmo.data import build_train_dataloader, MemMapDataset, DataCollator, IterableDataset\n",
    "import torch.distributed as dist\n",
    "from olmo.torch_util import barrier, get_global_rank, get_world_size\n",
    "\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, cast\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_memmap_dataset(\n",
    "    max_seq_len, memmap_dtype, pad_token_id, eos_token_id, paths: List[str], datasets: Optional[Dict[str, List[str]]] = None, include_instance_metadata: bool = True\n",
    ") -> MemMapDataset:\n",
    "    paths: List[str]\n",
    "    metadata: List[Dict[str, Any]] = []\n",
    "    if paths:\n",
    "        if datasets:\n",
    "            raise Exception(\"paths is mutually exclusive with datasets\")\n",
    "        paths = paths\n",
    "        for path in paths:\n",
    "            metadata.append({\"path\": str(path)})\n",
    "        print(len(metadata))\n",
    "    elif datasets:\n",
    "        paths = []\n",
    "        for label in sorted(datasets.keys()):\n",
    "            label_paths = datasets[label]\n",
    "            paths.extend(label_paths)\n",
    "            metadata.extend([{\"label\": label}] * len(label_paths))\n",
    "    else:\n",
    "        raise Exception(\"One of paths or datasets is required\")\n",
    "    return MemMapDataset(\n",
    "        *paths,\n",
    "        chunk_size=max_seq_len,\n",
    "        memmap_dtype=memmap_dtype,\n",
    "        metadata=metadata,\n",
    "        include_instance_metadata=include_instance_metadata,\n",
    "        pad_token_id=pad_token_id,\n",
    "        eos_token_id=eos_token_id,\n",
    "        generate_attention_mask=False,\n",
    "        generate_doc_lengths=True,\n",
    "        instance_filter_config=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_train_dataloader(\n",
    "    #train_config: TrainConfig,\n",
    "    device_train_batch_size,\n",
    "    pad_direction,\n",
    "    pad_token_id,\n",
    "    max_seq_len, \n",
    "    memmap_dtype,  \n",
    "    eos_token_id, \n",
    "    paths: List[str],\n",
    "\n",
    "    save_folder,\n",
    "    num_workers,\n",
    "    pin_memory,\n",
    "    prefetch_factor,\n",
    "    persistent_workers,\n",
    "    timeout,\n",
    "    epoch = 0,\n",
    "    drop_last = False,\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    *,\n",
    "    save_overwrite = False,\n",
    "    world_size: Optional[int] = None,\n",
    "    rank: Optional[int] = None,\n",
    "    fs_local_rank: Optional[int] = None,\n",
    "    include_instance_metadata: bool = False,\n",
    ") -> DataLoader:\n",
    "    assert device_train_batch_size is not None\n",
    "    collator = DataCollator(\n",
    "        pad_direction=pad_direction, pad_token_id=pad_token_id\n",
    "    )\n",
    "    dataset = build_memmap_dataset(\n",
    "        max_seq_len, memmap_dtype, pad_token_id, eos_token_id, paths, include_instance_metadata=include_instance_metadata\n",
    "    )\n",
    "    work_dir = Path(save_folder) / \"train_data\"\n",
    "    if get_global_rank() == 0:\n",
    "        if work_dir.is_dir() and not save_overwrite:\n",
    "            raise Exception(\n",
    "                \"train data working directory already exists, use --save_overwrite to overwrite\"\n",
    "            )\n",
    "        else:\n",
    "            work_dir.mkdir(exist_ok=True, parents=True)\n",
    "    if dist.is_available() and dist.is_initialized():\n",
    "        dist.barrier()\n",
    "    seed = 1324 #train_config.data.seed if train_config.data.seed is not None else train_config.seed\n",
    "    return DataLoader(\n",
    "        IterableDataset(\n",
    "            dataset,  # type: ignore\n",
    "            global_train_batch_size,\n",
    "            seed=seed,\n",
    "            epoch=epoch or 0,\n",
    "            shuffle=True,\n",
    "            drop_last=drop_last,\n",
    "            world_size=world_size,\n",
    "            rank=rank,\n",
    "            fs_local_rank=fs_local_rank,\n",
    "            work_dir=work_dir,\n",
    "        ),\n",
    "        batch_size=device_train_batch_size,\n",
    "        drop_last=drop_last,\n",
    "        collate_fn=collator,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=pin_memory,\n",
    "        prefetch_factor=None if num_workers == 0 else prefetch_factor,\n",
    "        persistent_workers=False if num_workers == 0 else persistent_workers,\n",
    "        timeout=timeout,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_train_batch_size = 2\n",
    "device_train_microbatch_size = 8\n",
    "device_train_batch_size = global_train_batch_size // get_world_size()\n",
    "pad_direction = \"right\"\n",
    "paths = [\"/n/holyscratch01/barak_lab/Lab/data/dolma-algebraic-stack-tokenized-llama/0/part-0-00000.npy\"]#glob(\"/n/holyscratch01/barak_lab/Lab/data/dolma-algebraic-stack-tokenized-llama/**/*.npy\")\n",
    "num_workers = 16\n",
    "drop_last = True\n",
    "pin_memory = True\n",
    "prefetch_factor = 16\n",
    "persistent_workers = True\n",
    "timeout = 0\n",
    "generate_doc_lengths = True\n",
    "pad_token_id = 1\n",
    "max_seq_len = 2048\n",
    "memmap_dtype = getattr(np, \"uint16\")\n",
    "eos_token_id = 2\n",
    "save_folder = \"./temp/\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.uint16'>\n"
     ]
    }
   ],
   "source": [
    "print(memmap_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/holylabs/LABS/sham_lab/Users/mkwun/envs/tmrc_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "train_loader = build_train_dataloader(\n",
    "    device_train_batch_size,\n",
    "    pad_direction,\n",
    "    pad_token_id,\n",
    "    max_seq_len, \n",
    "    memmap_dtype,  \n",
    "    eos_token_id, \n",
    "    paths,\n",
    "\n",
    "    save_folder,\n",
    "    num_workers,\n",
    "    pin_memory,\n",
    "    prefetch_factor,\n",
    "    persistent_workers,\n",
    "    timeout,\n",
    "    drop_last,\n",
    "    save_overwrite = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2048])\n",
      "(tensor([[3211,   13, 1678,  ...,   13,   13,  458],\n",
      "        [  12,   12,  361,  ..., 3027,  580, 1040]]),)\n",
      "(None,)\n",
      "(None,)\n",
      "(tensor([[2048,    0],\n",
      "        [ 534, 1514]], dtype=torch.int32),)\n",
      "([2048, 1514],)\n"
     ]
    }
   ],
   "source": [
    "for idx, batch in enumerate(train_loader):\n",
    "    if idx == 2:\n",
    "        print(batch[\"input_ids\"].shape)\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "        print(input_ids)\n",
    "        attention_mask=batch.get(\"attention_mask\"),\n",
    "        print(attention_mask)\n",
    "        attention_bias=batch.get(\"attention_bias\"),\n",
    "        print(attention_bias)\n",
    "        doc_lens=batch.get(\"doc_lens\"),\n",
    "        print(doc_lens)\n",
    "        max_doc_lens=batch.get(\"max_doc_lens\"),\n",
    "        print(max_doc_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "tensor([2048,  534, 1514], dtype=torch.int32)\n",
      "tensor([2048,  534, 1514], dtype=torch.int32)\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 2, 2, 2]])\n",
      "tensor([2048,  534, 1514])\n"
     ]
    }
   ],
   "source": [
    "print(doc_lens[0].shape)\n",
    "print(doc_lens[0].masked_select(doc_lens[0] != 0))\n",
    "# batch_doc_lens = torch.cat(\n",
    "#         [\n",
    "#             torch.tensor([0], dtype=torch.int32),\n",
    "#             doc_lens[0].masked_select(doc_lens[0] != 0)\n",
    "#         ])\n",
    "\n",
    "batch_doc_lens = doc_lens[0].masked_select(doc_lens[0] != 0)\n",
    "\n",
    "print(batch_doc_lens)\n",
    "\n",
    "batch_doc_mask = torch.cat([torch.full([e.tolist()], i) for i, e in enumerate(batch_doc_lens)]).reshape(device_train_batch_size, max_seq_len)\n",
    "print(batch_doc_mask)\n",
    "print(torch.bincount(batch_doc_mask.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch.nn.attention.flex_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_attention\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     _DEFAULT_SPARSE_BLOCK_SIZE,\n\u001b[1;32m      3\u001b[0m     create_block_mask,\n\u001b[1;32m      4\u001b[0m     create_mask,\n\u001b[1;32m      5\u001b[0m     flex_attention,\n\u001b[1;32m      6\u001b[0m )\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch.nn.attention.flex_attention'"
     ]
    }
   ],
   "source": [
    "from torch.nn.attention.flex_attention import (\n",
    "    _DEFAULT_SPARSE_BLOCK_SIZE,\n",
    "    create_block_mask,\n",
    "    create_mask,\n",
    "    flex_attention,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from doc lens, create doc mask in flex attention form"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
