defaults:
  - _self_

datasets:
  name: algebraic-stack
    path: /home/nikhilanand/algebraic-stack
  tokenizer_used: t5-base

model:
  _target_: Cfg
  n_heads: 8
  d_model: 2048
  n_layer: 8
  dropout_p: 0.0
  ctx_len: 512
  weight_precision: bfloat16

optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0001
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1e-8
  precision: float32

# scheduler:
#   _target_: torch.optim.lr_scheduler.CosineAnnealingLR
#   T_max: 1000
#   eta_min: 1e-6

training:
  epochs: 1
  train_steps: 500 # do whatever is larger, train_steps or epoch
  batch_size: 2048
  log_interval: 500
  shuffle: True
  save_model: True
  artifacts_path: /home/nikhilanand/Research/artifacts
  use_oracle: False

wandb_log:
  name: tmrc_dev_log202408

HydraConf:
  version_base: "1.1"